<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html>

<!-- PMJ 29/5/99 -->

  <head>
    <title>STAR Offline QA Documentation</title>
    <meta name="Author" content="Peter Jacobs">
    <meta name="KeyWords" content="STAR RHIC QA">
  </head>


<h2> STAR QA for Offline Software</h2>

Peter Jacobs, LBNL<br>
Sept. 27, 1999

<p> <em>Suggestion for reading this page: open this frame in another
window or use the mouse to raise the horizontal bar dividing the upper
and lower frame.</em>

<!---------------------------------------------------------------->

<h3> Index </h3>

<ol>

<li> <a href="#introduction">Introduction</a>
<ol>
<li> <a href="#what_is_qa">What is QA?</a>
<li> <a href="#overview">Overview</a>
</ol>

<li> <a href="#database_content">Database Content and Operations</a>
<ol>
<li> <a href="#what_is_database">What is the QA database?</a>
<li> <a href="#database_updates">QA database updates</a>
<li> <a href="#dataset_selection">Dataset selection and display</a>
</ol>

<li> <a href="#view_run_info">Viewing Run Information</a>
<ol>
<li> <a href="#view_dataset">Data Set</a>
<li> <a href="#view_created">Created/On disk?</a>
<li> <a href="#view_run_status">Run status</a>
<li> <a href="#view_qa_status">QA status</a>
</ol>

<li> <a href="#run_details">Run Details</a>

<li> <a href="#qa_details">QA Details</a>

<li> <a href="#files_and_reports">Files and Reports</a>

<li> <a href="#compare_similar_runs">Compare Similar Runs</a>

<li> <a href="#automated_qa">Automated QA and Automated Tests</a>
<ol>
<li> <a href="#automated_tests">Automated Tests</a>
<li> <a href="#control_and macro_def_files">Details of Control and Macro Definition files</a>
<li> <a href="#automated_qa_adding_new_macros">Adding new macros</a>
</ol>

<li> <a href="#current_scalars_and_tests">Current scalars and tests</a>

<li> <a href="#experts_page">Expert's page</a>

<li> <a href="#perl_comments">PERL, Object Oriented PERL, and CGI.pm</a>

</ol>

<!---------------------------------------------------------------->
<ol>
<!---------------------------------------------------------------->

<hr><h3><li><a name=introduction> Introduction </a></h3>

<ol>

<h4><li><a name=what_is_qa> What is Software QA?</a></h4>

The goal of Offline Software QA is the validation of the results of
large scale DST production and its associated prototype testing. This
requires examination of various aspects of both the software
infrastructure and physics algorithms used in DST production. QA is
not the same as physics analysis, but it should give confidence that
the DSTs that are being produced are valid for physics analysis. The QA
process should represent the collective knowledge of the content of the
DSTs, define what constitutes a deviation from known or expected
distributions of important quanities on the DST, and raise flags if
deviations are found. Deviations are not always bad, of course, and
can signal new physics: QA must be used with care in areas where there
is a danger of biasing the physics results of STAR.

<p>
STAR will produce hundreds of terabytes of data each year. Meaningful
testing of the DSTs produced from these data is a daunting task,
entailing an enormous amount of tedious repetition. This process must
be automated to a very high degree, for reasons both of reliability
and finite capacity of even the most dedicated grad student to do
boring but important things. The web pages you are looking at are the
first attempt at an automated framework for QA and testing.

<p>
There are three general areas that need to be addressed by QA:

<ol>

<li> Validation of nightly and weekly prototype production runs of a
limited number of events, and approval of library version migration
dev->new and new->pro based upon the results.

<li> Validation of large scale production running in new and
pro. Every event on every STAR DST should be subject to QA in a timely
fashion, in order to catch errors or unexpected changes in production
quickly.

<li> Maintenance of a framework for users who contribute code to the DST
production to test and validate their code privately in the same
environment that is used for 1. and 2.
 
</ol>

Currently, only the first point (nightly and weekly prototype testing)
is addressed by the Automated Offline Software QA framework. The
second point, the QA of large scale production, still requires
discussion about data organization and access, and what the actual
large scale QA tasks should be. The third point, providing a similar
environment for users in a private area, will be available soon.

<h4><li><a name=overview> Overview </a></h4>

The Offline Software QA framework consists of a set of <a
href="#perl_comments">CGI scripts written in PERL</a>, which
perform the following tasks:

<ol>

<li><font color=red>Data Catalogue:</font> Maintain a database of all
production and test runs, together with a performance summary of each
run: what was run, completed successfully or not, run time errors and
diagnostics, memory and cpu usage, etc. New runs are added to the
database by a scan of the disk volumes where test and production data
reside ( as of 13/9/99 this is restricted to /disk00000/star/test/).

<li><font color=red>Automated QA:</font> Run a set of standard QA ROOT
macros on the output data of the run, report the results, and
catalogue them in the database. The QA macros generate postscipt files
of histograms or ascii files containing scalars relevant to QA, both
event-wise and run-wise scalars. Which macros are run may depend upon
the nature of the data being analysed (real vs simulated, cosmic,
calibration, etc.) The scripts facilitate the comparison of different
runs by building comparison tables. An important planned functionality
is the automatic superposition of reference histograms by the QA
macros.

<li><font color=red>Automated Testing:</font> Run a set of tests on the
scalars generated by the QA macros, highlight scalars that fall
outside of expected ranges, and catalogue the results in the
database. The tests that are applied likewise can depend upon the
nature of the data being analysed, and the specific cuts of the tests
can vary as needed.

</ol>

<p>
The web pages present these data in a heirarchical fashion, with the
most important information most prominently displayed. Go to the upper
frame window, choose an interesting subset of all catalogued data from
the pull-down menu below "Select Datasets:", and press the "Display
Selected Datasets" button. The runs satisfying the selection are
listed in reverse chronological order, with details about the run
submission, status of the data on disk, and a very brief performance
summary, given in the first three columns. The "QA Status" and buttons
on the right hand side are described below.

<p>
The scalars and histograms are generated by ROOT macros running in the
standard STAR framework. The CGI scripts set up the running of these
macros and submit them as background processes, currently on the web
server. (Submission of these processes as batch jobs on CAS is a
straightforward extension of this mechanism.) These macros include the
familiar ones written by Kathy Turner and collaborators, which have
served as the main QA tools until now (QA_bfcread_dst_tables.C,
bfcread_hist_to_ps.C). These will continue to be used for QA, with the
CGI scripts taking care of the tedious mechanical tasks of running the
macros and cataloguing the results in an organized fashion. In
addition, doEvents.C is now run on all events and QA-related
quantities are reported about the run as a whole (mean, rms, min and
max tracks per event, for instance).

<p>
The framework has been written so that the addition of new macros is
straightforward. No changes to the CGI scripts are needed to introduce
new macros which produce postscript files, and a single PERL
subroutine needs to be added for a new macro which generates an ascii
file, to parse that file, extract the QA scalars and put them into
some defined PERL structures. New QA macros should be developed and
run in this framework for more detailed and targeted QA, including QA
that is specific to a particular kind of data (peripheral collisions,
calibrations, etc.) The processes that are specific to a certain
flavour of data are controlled by a steering mechanism that is
outlined below.

<p>
Two kinds of QA scalars are considered: <font color=red>
run-based</font> and <font color=red> event-based</font>. The
run-based scalars characterize the run as a whole (for instance, the
mean, rms, minimum and maximum number of tracks per event for a run of
40 events). The event-based scalars characterize each individual event
(the number of TPC hits in each event, is such-and-such a table is
present in this event, etc.) As has been pointed out by a number of
people, the "scalars" may also be the result of statistical tests
(such as a chisquared or Kolmogorov test) comparing a histogram from
the selected run to a reference histogram.

<p>
In addition to running QA ROOT macros to generate the scalars and
histograms, the Offline Software QA framework can apply Boolean tests
to an arbitrarily large set of scalars generated by these
macros. (This is defined above as <font color=red>Automated
Testing</font>.) These tests will be of greatest use in probing the
self-consistency of large scale production, but can also be used to
detect changing conditions in the nightly and weekly test
results. Such a procedure raises the possibility of introducing biases
into the analysis, and the failure of tests should in some cases be
considered simply a flag of a situation that warrants closer
inspection. In other cases (such as required data missing or
identically zero), they will flag conditions that are obviously errors
and require attention. In either case, an automated test system is
essential for scanning any significant amount of data.

<p>
The tests compare the scalars to externally specified cuts, and
failure of a test is reported as an "error" or a "warning" (severity
specified as part of the test definition.) The appropriate cuts may
depend upon the specific nature of the data being analysed (for
example, the expected distribution of number of tracks per event is
quite different for central trigger and minimum bias trigger
conditions).

<p>
The results of all tests for each QA macro applied to a given run are
summarized in the run listing table under "QA Status" . Details about
the scalars and tests can be displayed via the "QA details" button
(explained further below). There is a large amount of data to present,
not all of it always interesting, and this display will most likely
undergo changes. Suggestions to improve the clarity of this display
and make it more intuitive are especially welcome.
 
<p>
The time dependence of QA scalars can be viewed via the "Compare
similar runs" button. This currently shows tables of the run-based
scalars for all macros that have been applied to the selected run,
comparing the 10 youngest runs having (currently) the same geometry
and TPC simulator as the selected run. An important future extension
of this will be to develop ROOT macros to superimpose histograms from
a reference run on the QA histograms for each selected run.

<p>
Functionality that modifies the database (performing updates, running
the QA macros, etc.) is hidden in a password-protected <a
href="#experts_page">Expert's page</a>.

</ol>

<!---------------------------------------------------------------->

<hr><h3><li><a name="database_content">Database Content and Operations</a></h3>

<ol>

<h4><li>
<a name="what_is_database">What is the QA database?</a>
</h4>

The QA "database" is a UNIX directory tree (currently
<tt>/star/data1/jacobs/qa/reports</tt>), consisting of a series of
"report" and "evaluation" files for each test run that has been
analysed. Each run is assigned a <font color=blue>database key</font>,
such as <font color=blue>dev.tfs_Linux.Tue.year_2a.140999</font>
(production was run on Sept 14, 1999), and which serves as the name of
the subdirectory containing all files related to this run. There are
several types of files in each subdirectory. The casual user need not
know anything about these files: an important function of the CGI
scripts is to present their contents to the user's web browser in
digestible ways. However, for completeness, the files are:

<ul>

<li> <tt>logfile_report.txt</tt>: summary of the run, generated by
parsing the log file when the run is entered into the database. It is
present for all runs in the database.  

<li> files of type <tt>.qa_report</tt>: Ascii file generated by each
QA macro that produces acsii output of QA information. Filename is
name of the macro. (doEvents currently sends QA information to STDOUT,
so that doEvents.qa_report is in fact generated by parsing the STDOUT
stream to extract this QA information.)

<li> files of type <tt>.ps.gz</tt>: Gzipped versions of postscript
files generated by QA macros such as bfcread_hist_to_ps. Filename is
name of macro. Links to these files are presented to the browser when
QA Details are viewed. Gzip reduces the file size by a factor 8 and
decreases the time to open the file in ghostview on a remote browser
by a large factor.

<li> files of type <tt>.evaluation</tt>: The result of Automated
Testing applied to the qa_report output of one QA macro. This is a
PERL Object, written using the PERL module "Storable" (see the section
on <a href="#perl_comments">PERL</a>), and is not ascii and therefore
not readable outside of a PERL script.

</ul>

<h4><li>
<a name="database_updates">QA database updates</a>
</h4>

Updating the QA database is carried out manually from the <a
href="#experts_page">Expert's page</a> or by means of a cron job (see
your favourite UNIX manual). Date and time of the last update are given
in the upper panel, under the "Display selected dataset" button. If an
update job is in progress, blue text will indicate that immediately
below this message.

<p> The updating process examines all directories in a specified disk
area to find and catalogue production runs that have completed since
the last update. The search is currently carried out on
<tt>/disk00000/star/test/dev/</tt> and
<tt>/disk00000/star/test/new/</tt>, and will change or expand as
appropriate. Care must be taken not to catalogue runs that are in
progress. Runs that have completed successfully are straightforward to
identify, but runs that crash can do so in a variety of ways, and a
robust means to distinguish these from runs in progress is not yet in
place. Consequently, some runs that crash or hang may not be
identified by the current updating mechanism, though this appears to
be rare.

<h4><li>
<a name="dataset_selection">Dataset selection and display</a>
</h4>

Catalogued runs are grouped by common properties (currently the
grouping criteria are library version, TPC simulator, platform, and
geometry). Subsets of the full database can be displayed using the
pull-down menu under "Select datasets:" and pushing the "Display
selected dataset" button. Catalogued runs corresponding to the
selection are listed below the horizontal line in the upper
frame. Selecting "All archived datasets" may produce a very long list.

</ol>

<!---------------------------------------------------------------->

<hr><h3><li><a name="view_run_info">Viewing Run Information</a></h3>

The run display in the upper frame has five columns, labelled "Data
Set", "Created/On disk?", "Run Status", "QA Status", and an unlabelled
column containing action buttons which are described in detail in
following sections. <font color=green>This display can be refreshed at
any time by pushing the "Display selected dataset" button.</font>
(This is useful if you happen to view the page while a QA batch job is
being run. You will need to refresh the display in order to see the
results of the job. Reloading the web page into your browser to
refresh the page also works, but it takes longer.)

<ol>

<h4><li>
<a name="view_dataset">Data Set</a>
</h4>

Displays the directory where the production run resides (or
resided) on disk, the STARLIB version and STAR Library level, as
extracted from the log file, and the filename of the input data used
for the run.

<h4><li>
<a name="view_created">Created/On disk?</a>
</h4>

Displays the date and time of submission of the run and
whether it resides on disk at the moment the script runs. Since
the same directory names are used week-by-week for the nightly
production files, the directory name does not uniquely identify the
run and additional checks are made to ensure consistency between the
QA database entry and the data on disk.

<h4><li>
<a name="view_run_status">Run status</a>
</h4>

Displays basic performance-related information about the run,
extracted from the log file. Reports whether the run completed
succssfully or crashed, whether a segmentation fault occurred, the
number of events completed, the number of events requested, and the
specific event sequence requested from the input file (SL99h and
later). Reports whether all data files that should be generated by the
production chain are present in the directory. (The file of type
.hist.root might exist but not contain the relevant histograms, which
is indicated by its size being much smaller than a correct .hist.root
file. This condition is also reported as a missing .hist.root file.)

<h4><li>
<a name="view_qa_status">QA status</a>
</h4>

If a production run has been added to the catalogued but the QA macros
not yet applied, displays "QA not done". Otherwise, displays the date
and time that QA was carried out, and succint information from each QA
macro that was run. Reports if macro crashed (this should be rare). If
<a href="#automated_tests">Automated Testing</a> has been done,
reports "o.k." if all tests passed, otherwise reports number of errors
and warnings generated.

<p> If QA for this run alone was initiated (from the <a
href="#experts_page">Expert's page</a>), blue text will indicate that
a QA batch job is in progress.

</ol>

<!---------------------------------------------------------------->

<hr><h3><li>
<a name="run_details">Run Details</a>
</h3>

Shows detailed run information extracted from the log file: run setup
information, including the options selected; CVS tags of code used;
and CPU usage. Memory usage can also be included but is not currently
extracted from the log file. Some of this information may be absent if
the run did not complete correctly.

<!---------------------------------------------------------------->

<hr><h3><li>
<a name="qa_details">QA Details</a>
</h3>

This button displays the QA scalars and links to histogram files
generated by all QA macros, along with results from the Automated QA
testing. There is a large volume of information to present here, of
varying general interest and importance.  The run-based scalars tend
to be more important for physics QA than the event-based scalars, and
so are highlighted in the order of display. QA tests that fail are
likewise highlighted over those that succeed. The display in the lower
frame is divided into several sections:

<ol>
<li> Control and Macro Definition files: links to the specific <a
href="#control_and macro_def_files">control and macro definition
files</a> used for QA. Physical location of the files is given for
reference, but clicking on the link will open the file. These files
define the QA macros to be run, the run and event based scalars to
extract, and the automated QA tests and specific cuts to apply. Each
run has one control file and one or more macro definition files, which
may be valid only for a specific event type (central collisions,
cosmics, etc.), time period, or sequence of library versions

<li> QA histograms: links to all .ps files generated by the QA macros
that have been run. This currently consists solely of Kathy's standard
QA histograms generated by bfcread_hist_to_ps, but is expected to
expand. Format is gzipped postscript. Physical location of the files
is given for reference, but clicking on the link will open the
file. This should open quickly even at remote sites. Optimally, your
browser should be configured to open .ps files using a recent version
of ghostview that automatically gunzips a gzipped file.

<li> Run-based scalars, errors and warnings: Run based scalars (see <a
href="#overview">overview</a>) are presented for each QA macro for
which they are defined, together with the <a
href="#automated_tests">automated tests</a> that failed and generated
an error or warning. The tests are labelled by short strings and are
defined in more detail farther down the display. See <a
href="#current_scalars_and_tests">current scalars and tests</a>.

<li> Event-based errors and warnings: Same as previous section, but
for event-based scalar tests that generated errors and warnings. The
actual scalar values are not given here. Event-based scalars are
tabulated for each event and there may be many of them in total. Their
values can be found in the tables of all QA tests applied, farther
down the display.

<li> Run-based tests (all entries): displays all QA tests applied to
run-based scalars for each macro. Display shows each boolean test
string, severity if failed (error or warning), and result (TRUE or
FALSE). Failed tests are highlighted in red.

<li> Event-based tests (all entries): displays all QA tests applied to
event-based scalars for each macro. Display shows each boolean test
string, severity if failed (error or warning), and result (TRUE or
FALSE). Failed tests are highlighted in red.

</ul>

<!---------------------------------------------------------------->

<hr><h3><li>
<a name="files_and_reports">Files and Reports</a>
</h3>

The table shows all files in the given production directory,
together with their size and date of creation.
<p>
The section labelled "Reports for..." displays: 

<ol>

<li> Logfile: a link is given to the log file for the run. Check the
size of the logfile before opening it: the largest log files can
exhaust the virtual memory of your PC.

<li> Postscript files: links are given to all postscipt files
generated by the QA macros. These are the same as the links given
under "QA histograms" on the "QA details" page.

<li> Other files: shown only on the <a href="#experts_page">Expert's
page</a>. All files (other than of type .ps.gz) that are in the <a
href="#what_is_database">QA Database</a> subdiretory for this run are
displayed. Links are provided to ascii files (this excludes files of
type .evaluation).

</ol>

<!---------------------------------------------------------------->

<hr><h3><li>
<a name="compare_similar_runs">Compare Similar Runs</a>
</h3>

This display is proving to be the most useful for first-pass QA. The
run-based scalars for the selected run are compared to those from up
to 10 of the most recent "similar" runs in the QA database. "Similar"
currently means the same TPC simulator (tfs, tss or trs) and geometry
(year_1b or year_2a).

<p>
All comparisons runs are listed in a table and are assigned an
arbitrary letter label for convenience. The remaining tables show a
comparison of run-based scalars for each of the QA macros that was
applied to the selected run, first the difference in value of each
scalar relative to that for the selected run, and then the absolute
values themselves (these tables obviously display the same
information). Comparison runs with no valid entries for a given scalar
value (macro wasn't run or it crashed) do not appear. If only some
entries are valid, the remainder are given as "undef".

<p> 
For convenience when writing the QA summary reports, the tables in
this display are also written to an ASCII file, whose name is given
near the top of the display.

<p>
A minor expansion of this capability will be to select a single
reference run for comparison. In the longer term, a histogram
comparison facility will be developed, automatically plotting the QA
histograms for both a selected run and a reference run on the same
panel.

<!---------------------------------------------------------------->

<hr><h3><li>
<a name="automated_qa">Automated QA and Automated Tests</a>
</h3>

In this documentation, I have tried to use the following definitions
consistently (see also <a href="#overview">overview</a>):

<ul>
<li><font color=red>Automated QA:</font> Running a set of QA root
macros on the production files.
<li><font color=red>Automated Testing:</font> Apply a set of cuts to
the QA scalars generated by these macros.
</ul>
While these are separate, sequential operations in
principle, in practice in the current Offline Software QA framework
they are specified together. After a discussion of automated tests, I
will discuss the steering mechanism and how both the QA and testing
are specified for a given macro.

<p>
The appropriate set of tests to apply, and in the case of numerical
comparisons, the actual values to compare to, are often dependent upon
the specific class of event under consideration. Simulated events from
event generators require different tests than cosmic ray events, and
there will of course be many different classes of real data with
different properties. The selection of the appropriate set of QA
macros and tests to apply to a given run is done by means of a
"Control File", which specifies "Macro Definition files", one for each
QA macro to be applied. Each event class has a Control File.  The
detailed format and syntax of these files is discussed below.

<p>
This selection mechanism is currently rather primitive: the name of
the control file is hardwired in the CGI script, and the selection
depends only upon the geometry (year_1b or year_2a). This mechanism
needs to be integrated into the mainstream STAR data handling
framework, and will develop in that direction soon. In addition to
dependence upon class of event, the control file will also depend upon
library version and date of production and of QA.

<p>
The setting of cuts and the testing could be done in ROOT processes -
this, after all, is a basic functionality of analysis frameworks such
as PAW and ROOT. However, the manpower to integrate this into the
framework is not currently in sight.

<ol>

<h4><li>
<a name="automated_tests">Automated Tests</a>
</h4>

Examples of tests on QA scalars are:

<ul>
<li> is Table X present in each event on the dst?
<li> are there tables present in any event that are not expected for
this class of data?
<li> is the number of entries for Table X equal to those for Table Y
in all events?
<li> is the mean number of tracks per event within a given window for
this run?
</ul>

There may be tests on both run-based and event-based quantities. In
some cases a simple binary quantity in tested (table present or not),
in other cases a numerical comparison is made. The section on <a
href="#qa_details">QA Details</a> describes how to view the results of
the QA tests.

<h4><li>
<a name="control_and macro_def_files">Details of Control and Macro Definition files</a>
</h4>

<ul>
<li> Control File: This file contains a set of names of Macro
Definition files, one per line. Lines beginning with a pound sign
("#") are comments and blank lines are ignored. All other lines are
understood to contain a file name, which should begin in the first
column. As an example:
<pre>
# control file for year 1b tests
# lines with "#" in first column are comments
# all other non-blank lines should contain a test file with full pathname for one macro
#--------------------------------------------------------------------------------
/star/u2/jacobs/QA/cgi/development/control_and_test/QA_bfcread_dst_tables.year_1b.v1.test
/star/u2/jacobs/QA/cgi/development/control_and_test/bfcread_hist_to_ps.v1.test
/star/u2/jacobs/QA/cgi/development/control_and_test/doEvents.year_1b.v1.test
</pre>
is the current control file for runs with year_1b geometry. The QA macros
QA_bfcread_dst_tables, bfcread_hist_to_ps and doEvents will be run.

<li> Macro Definition files: Here is (part of) the file
QA_bfcread_dst_tables.year_1b.v1.test refered to by the example in the
previous bullet:
<pre>
macro name: $STAR/StRoot/macros/analysis/QA_bfcread_dst_tables.C
macro arguments: nevent=1 infile outfile
input data filetype: .dst.root
first starlib version: SL99a
last starlib version: SL99z
macro comment: Test of First Event Tables
end of header:
run scalars: globtrk globtrk_aux globtrk2 primtrk primtrk_aux 
run scalars: vertex dst_v0_vertex ev0_eval dst_xi_vertex kinkVertex
run scalars: particle
run scalars: point dst_dedx g2t_rch_hit
run scalars: TrgDet
run scalars: event_header event_summary monitor_soft
BEGIN TEST:
run test name: Table exists
test comment: Check that table is present in first event
error: globtrk .found.
error: globtrk_aux .found.
... some text removed ...
error: dst_dedx .found.         
error: monitor_soft .found.             
END TEST:
BEGIN TEST:
run test name: Row in range
test comment: Check that number of rows within reasonable range
error: globtrk .gt. 8600
error: globtrk .lt. 9000
error: globtrk_aux .eq. globtrk
... some text removed ...
error: monitor_soft .eq. 1              
error: dst_dedx .eq. globtrk            
END TEST:
BEGIN TEST:
run test name: Unexpected Tables
test comment: Check for unofficial tables present (not declared as test scalars)
error: nonscalar .notfound.
END TEST:
</pre>
</ul>
The full file can be seen by following the appropriate link under "QA details".

<p> 
The file is divided into:

<ul>
<li> a mandatory header, which defines the macro name, arguments,
input data filetype, valid starlib versions (not yet checked against),
and a further comment that will be displayed on the "QA Details"
page. In the macro name, note the presence of the environment variable
<t>$STAR</tt>. If this is given rather than an absolute path name, the
macro will be run under the same library version as the production was
run (the version is extracted from the log file).

<li> an optional declaration of expected scalars. In this case only
run-based scalars are declared, but event-based scalars can be
declared in a similar way. (The fact that this particular macro only
looks at the number of rows for each table in the first event, yet
these quantities are called "run-based scalars", is very confusing. An
attempt at an explanation of this curiousity is given in a following
section.)  The appropriate scalars must be declared if a test is
defined.

<li> optional test definitions. Tests can be "run tests" (for testing
run-based scalars) or "event tests" (for testing event-based
scalars). Test name is mandatory, comment is optional. The Boolean
tests are given one per line. A line such as
<pre>
error: globtrk .lt. 9000
</pre>
is understood as testing that the scalar value "globtrk" (the number
of global tracks in the first event) is less than 9000; if it
is not, an error is reported. In other words, the
format is <tt>severity: string</tt>, where a failure (with the given
severity) is reported if
the string is false.

</ul>

<p> There are special cases built in (such as the scalar
<tt>nonscalar</tt> appearing in the last test of the example), some of
which are proving to be more useful than others. I will not try to
give a full specification of the test language here - that would
quickly become obsolete. This "metalanguage" is of course defined by
the CGI scripts, and it will change and develop as needed (insert the
usual boilerplate about backward compatibility here). If the existing
examples are not sufficient for your application, you should contact
me, but if you are at that level of detail, you probably already have
done so.

<h4><li>
<a name="automated_qa_adding_new_macros">Adding new macros</a>
</h4>

The steps currently needed to add a new macro are:
<ul>
<li> Create the appropriate 
<a href="#control_and macro_def_files">macro definition files</a>
for the new macro.

<li> Modify the <a href="#control_and macro_def_files">control
files</a> for the relevant event types.

<li> If scalars are defined, modify the PERL module
<tt>QA_macro_scalars.pm</tt> by adding a subroutine with the same name
as the macro. This subroutine should parse the report file generated
by the macro (file of type <a href="#what_is_database">.qa_report</a>)
and return references to two hashes containing the scalars (this is
PERL-speak):
<pre>
sub new_macro_name{ 
  %run_scalar_hash = ();
  %event_scalar_hash = ();
... do some stuff...
  return \%run_scalar_hash, \%event_scalar_hash;
}
</pre>
A minimal example is <tt>sub QA_bfcread_dst_tables</tt> in
<tt>QA_macro_scalars.pm</tt>, but if you don't understand PERL, you
won't understand the example. A more complex example is  
<tt>sub doEvents</tt> in the same file, but the complexity of this routine is
driven by the comparison of QAInfo reported to STDOUT by doEvents to
the same lines in the logfile for the same events, and this is not
applicable to other macros.
</ul>

<p>
The PERL scripts have been almost completely decoupled from the
specific content of the existing QA macros, making the addition of new
macros to the QA processing rather easy. As noted, the only PERL
programming needed to add a new macro is the addition of a single
subroutine to <tt>QA_macro_scalars.pm</tt>, and only in the case that
scslars are defined (i.e. a new macro that only generates a postscript
file of histograms does not require any change to the CGI scripts). A
flexible and general interface between the QA ROOT macros and the QA
CGI scripts can be defined, so that no changes to the scripts would be
needed to add a new macro in all cases. This simply requires agreement
on the sequence and format of information contained in the QAinfo
lines in the ascii file. However, the above procedure already
presents a rather low barrier to introduction of new macros.

</ol>


<!---------------------------------------------------------------->
<hr><h3><li>
<a name="current_scalars_and_tests">Current scalars and tests</a>
</h3>

The current (Sept 99) scalars and Automated QA tests that are applied are for each macro are:

<ul>
<li> doEvents: 
<ul>
<li>Run-based: Run-based scalars are quantities such as
<tt>tracks_mean, tracks_rms, tracks_min</tt> and <tt>tracks_max</tt>,
which are the mean and rms of the number of tracks per event, and the
minimum and maximum per event over the run. See a Macro Definition
file for doEvents for the full list. There is one test on these
scalars, called <font color=green>Run stats in range</font>, which
checks each scalar against expected values.

<li> Event-based: No event-based scalars are defined. However, an
event-based test called <font color=green>Compare to Logfile</font>,
that is particular to doEvents, is defined. This test checks the
strings written by the message manager in doEvents ("qa" = "on")
against those written in the same event to the logfile during
production. The test is that the number of tracks, vertices, TPC
hits, etc., reported during production are the same as those read from
the DST by doEvents, verifying the integrity of the DST and the
reading mechanism. You will be pleased, but not surprised, to learn
that no errors of this kind have been detected, but we will continue
to check.
</ul>

<li> QA_bfcread_dst_tables: QA_bfcread_dst_tables is Kathy's standard
QA macro, which reports the number of rows for each table in the first
event of the run. These scalars are defined as run-based for the QA
tests. (<i>Why, you might wonder, since these scalars describe only
the first event?  On the other hand, we are using these scalars to
characterize the run as a whole: which tables are present, are the
physical values at least in the right ballpark? In addition, run-based
scalars are highlighted preferentially in the "QA Details" display,
and are the only scalars shown in the "Compare similar runs"
display. Perhaps this points out a failure in terminology, but run
scalars these will stay. Suggestions to improve the
terminology are welcome</i>). Tests on these scalars are:

<ul>
<li> <font color=green>Table exists</font>: checks that all tables that are officially expected for a given
class of event (year_1b vs. year_2a) are present in the first event
<li> <font color=green>Row in range</font>: similar to test "Run stats
in range" in doEvents, and highlights same problems. Checks that the number
of rows for each table in the first event of the run is within some
window or equal to the number of rows of another table.
<li> <font color=green>Unexpected Tables</font>: checks that there are
no tables in the first event that are <em>not</em> officially expected.
</ul>

</ul>

<!---------------------------------------------------------------->
<hr><h3><li>
<a name="experts_page">Expert's page</a>
</h3>

The Expert's page is a password protected page, containing the
standard display plus functions that affect the content of the
database. These all appear in the upper frame.

<ul>

<li> Global Actions:

<ul>

<li> Update catalogue: scans data disks for new runs, parses log files
and adds them to the catalogue. Does this online, with the status of
new runs found reported in lower frame. QA is not run on the new files
(this should be done in a batch job). All newly found runs that are
not yet in database are reported in the lower frame, but those runs
which cannot be classified as having finished are not added to the
database; this fact is reported in red text. In some cases, a job may
have crashed in a way not expected by the scripts. It is worthwhile
occassinally to grab the red logfile names with a mouse and "tail"
them in a UNIX window, to verify that indeed they have not
terminated. Notify me if you think there is a malfunction.

<li> Update catalogue and QA: Launches a batch job to perform same
function as previous item, but in addition runs QA on all newly
catalogued runs. Currently this batch job is an "at" job (see UNIX
manual) running on the server, but this same mechanism can be adapted
to submit batch jobs to CAS. The lower panel shows the result
of the unix command <tt>atq</tt> run under user <tt>starlib</tt> and
looks something like:

<pre>
Submitting batch job for catalogue update and global QA... 

Job submitted, status = 0 
Here is atq: 

 Rank     Execution Date     Owner     Job         Queue   Job Name
 
  1st   Sep 17, 1999 12:39   starlib 937586349.a     a     batch
</pre>

The only purpose of this display in the lower frame is to demonstrate
that the job was queued successfully. Once the job starts to execute
it will disappear from this queue. While it is running, blue text will
appear in the upper frame under "Last Catalogue Update...", and this
test will disappear when the job has completed. (Update the display
using the "Display selected dataset" button). The job can be seen
under any userid by doing <br><tt> ps -ef | grep starlib
</tt>.

<li> Server log: Shows the last 200 lines of the Apache web server
log, in reverse chronological order. Useful for script debugging (some
error messages go there). Worth checking for diagnostics in case of
peculiar behaviour of scripts.

<li> Server batch queue: Result of the unix command <tt>atq</tt> run
under user <tt>starlib</tt> (see example above).

<li> Batch logfiles: Links are given to all files in the directory
containing the logfiles from the batch jobs, in reverse chronological
order. Logfiles for jobs in progress are displayed separately. Files are
currently not named in any meaningful way, but the date of creation is
given and the operator should be able to tell which are relevant. For
jobs doing catalogue updates, see comments a few bullets up on
"Updating Catalogue".

<li> Run csh script: Catchall functionality to allow the operator to
perform various functions as user <tt>starlib</tt>. Does not accept
shell commands for security reasons. Operator gives name of file (with
full pathname) of type .csh that is executable by user
<tt>starlib</tt>, which contains the commands to be executed. Can be
used to kill processes, move files, etc.

<li> Do QA on whole dataset: submits batch job to carry out QA on all
runs in current dataset listing for which "QA not done". 

<li> Redo QA on whole dataset: submits batch job to delete <a
href="#what_is_database">all QA and evalutation-related files</a> on
all runs in current dataset listing, and redoes QA for all of
them. Think twice before doing this.

</ul>

<li> Run-specific actions:
<ul>
<li> Redo Evaluation: deletes files of type <a
href="#what_is_database">.evaluation</a> for selected run and redoes
Automated Testing (evaluation).
<li> Redo QA (batch): submits batch job to delete <a
href="#what_is_database">all QA and evalutation-related files</a> for
selected run and redo QA.

</ul>
</ul>


<!---------------------------------------------------------------->

<hr><h3><li>
<a name="perl_comments">PERL, Object Oriented PERL, and CGI.pm</a>
</h3>

"CGI" stands for "Common Gateway Interface", and refers to the
standard internet protocol for dynamically generating web pages by
running "CGI scripts" which respond to user actions. When you purchase
a book from Amazon.com, the web server is running a CGI script that
responds to your search and purchase requests and sends your purchase
details to literally hundreds of web-based merchants, who then target
their advertising banners straight at you. CGI scripts can be written
in various languages, but <a href="http://www.perl.com/">PERL</a> is a
well established industry standard for writing CGI scripts, is freely
distributed, has extensive documentation, support and software
available over the web and from the standard sources, and appears to
be the right choice for writing the QA CGI scripts.

<p>
PERL is an outgrowth of the UNIX csh and ksh scripting languages you
may be familiar with. It is an interpreted language, and among its
other uses it is very suitable for writing the kinds of scripts that
used to be written in csh and ksh. PERL scripts are also in principle
portable beyond UNIX, though that in fact depends upon how you write
them. PERL is much more elegant, intuitive, and pleasant to write and
read than csh or ksh, and has some very clever features that can make
the meaning of the code quite transparent (if you speak the lingo). In
addition, it has a very nice Object Oriented extension that I found to
be absolutely essential in writing the QA CGI scripts. The overhead to
learn OO programming in PERL is rather modest.

<p>
I found two books to be indispensable in learning PERL and writing PERL scripts,
both published by O'Reilly (the "In a Nutshell" people):
<ul>
<li> Programming PERL, by Wall, Christiansen and Schwartz
<li> The PERL Cookbook, by Christiansen and Torkington
</ul>

The first is the standard reference (not quite a language definition)
with a very useful pedagogical introduction, whereas the second
contains explicit recipes for many essential tasks that would take a
lot of time to figure out otherwise. Using it is a good way to learn
the PERL idioms, I think.  Surprisingly, the "PERL in a Nutshell" book
was not as useful as these books.

<p>
An extensive PERL module has been developed for writing CGI
scripts. It is called <a
href="http://stein.cshl.org/WWW/software/CGI">CGI.pm</a>, written by
Lincoln Stein at Cold Spring Harbor Lab, just down the road from
BNL. I also found this to be extremely valuable: it hides all the html
details behind a very convenient interface, allowing you to, for
instance, generate quite complex tables in a few program lines. The
scripts are very much cleaner and have fewer bugs as a result. The
CGI.pm web page gives a tutorial and extensive examples. There is a
book by Stein called "Official Guide to Programming with CGI.pm",
published by Wiley. I found it to be of less use than his web
page. Lousy businessman (that's why he's doing web programming for a
living at a biology lab).

<!---------------------------------------------------------------->
</ol>
<!---------------------------------------------------------------->

<!---------------------------------------------------------------->

<hr>


<!--===============================================================-->
    <hr noshade="noshade">
    <address><a href="mailto:pmjacobs@lbl.gov">webmaster</a></address>
    <!-- Created: Wed May  6 11:37:01 EDT 1998 -->
<!-- hhmts start -->
Last modified: Mon Sep 27 14:23:15 EDT 1999
<!-- hhmts end -->
<!--===============================================================-->
</body>
</html>
